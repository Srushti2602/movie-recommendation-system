# -*- coding: utf-8 -*-
"""big data ml-1m.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Xq8v-o-PZ-0bk63bbxesVAATD7huJa_

Movie Recommendation System

1.	Data Preprocessing: The code loads movie ratings, encodes user and movie IDs, normalizes the ratings, and splits the data into training and testing sets. These are converted into PyTorch datasets for model input.
2.	Model Definition and Training: A Neural Collaborative Filtering (NCF) model is defined, embedding user and movie features, and is trained using mean squared error loss over multiple epochs to predict normalized ratings.
3.	Model Saving: After training, the modelâ€™s weights are saved to a file (ncf_model.pth) for later use in generating recommendations.

# 1.Model train and test using ncf , MSE loss function and pyspark
"""

pip uninstall torch torchvision torchaudio -y

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

"""Code below is using pandas but the later one is using pyspark"""

import os
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from torch import nn, optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Step 1: Load and Preprocess Data
ZIP_FILE = "ml-1m.zip"
DATA_PATH = "ml-1m"

if not os.path.exists(DATA_PATH):
    os.system(f"unzip -n {ZIP_FILE} -d .")

# Load ratings data
ratings = pd.read_csv(
    os.path.join(DATA_PATH, 'ratings.dat'),
    delimiter='::',
    engine='python',
    header=None,
    names=['UserID', 'MovieID', 'Rating', 'Timestamp']
)

# Encode UserID and MovieID
user_encoder = LabelEncoder()
movie_encoder = LabelEncoder()
ratings['UserID'] = user_encoder.fit_transform(ratings['UserID'])
ratings['MovieID'] = movie_encoder.fit_transform(ratings['MovieID'])

n_users = ratings['UserID'].nunique()
n_movies = ratings['MovieID'].nunique()

# Normalize ratings
ratings['Rating'] = ratings['Rating'].astype(np.float32) / 5.0

# Split data into training and test sets
train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)

# Step 2: Create a PyTorch Dataset
class MovieLensDataset(Dataset):
    def __init__(self, data):
        self.users = torch.tensor(data['UserID'].values, dtype=torch.long)
        self.movies = torch.tensor(data['MovieID'].values, dtype=torch.long)
        self.ratings = torch.tensor(data['Rating'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        return self.users[idx], self.movies[idx], self.ratings[idx]

train_dataset = MovieLensDataset(train_data)
test_dataset = MovieLensDataset(test_data)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Step 3: Define the NCF Model
class NCF(nn.Module):
    def __init__(self, n_users, n_movies, embedding_dim=32):
        super(NCF, self).__init__()
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2, 128)
        self.fc2 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 1)
        self.activation = nn.Sigmoid()

    def forward(self, user, movie):
        # Get user and movie embeddings
        user_emb = self.user_embedding(user)  # Shape: [batch_size, embedding_dim]
        movie_emb = self.movie_embedding(movie)  # Shape: [batch_size, embedding_dim]

        # If user and movie dimensions do not match, expand user tensor
        if user_emb.shape[0] != movie_emb.shape[0]:
            user_emb = user_emb.expand_as(movie_emb)

        # Concatenate user and movie embeddings
        x = torch.cat([user_emb, movie_emb], dim=1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.sigmoid(self.output(x))

# Initialize the model
embedding_dim = 32
model = NCF(n_users, n_movies, embedding_dim)
criterion = nn.MSELoss()  # You can also try binary cross-entropy for implicit feedback
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Step 4: Train the Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def train_model(model, train_loader, criterion, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for user, movie, rating in train_loader:
            user, movie, rating = user.to(device), movie.to(device), rating.to(device)
            optimizer.zero_grad()
            output = model(user, movie).squeeze()
            loss = criterion(output, rating)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}")

train_model(model, train_loader, criterion, optimizer)

# Step 5: Save the Trained Model
torch.save(model.state_dict(), "ncf_model.pth")
print("Model saved as ncf_model.pth")

"""Next 6 cells do the same job of training using ncf but just using pyspark"""

pip install pyspark

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Test").getOrCreate()
print(spark.version)

#processing using pyspark
import os
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from torch import nn, optim
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from sklearn.preprocessing import LabelEncoder

# Initialize Spark
spark = SparkSession.builder.appName("MovieRecommendation").getOrCreate()

# Step 1: Load and Preprocess Data Using PySpark
ZIP_FILE = "ml-1m.zip"
DATA_PATH = "ml-1m"

if not os.path.exists(DATA_PATH):
    os.system(f"unzip -n {ZIP_FILE} -d .")

# Load ratings data with PySpark
ratings = spark.read.csv(
    os.path.join(DATA_PATH, 'ratings.dat'),
    sep="::",
    inferSchema=True,
    header=False
).toDF("UserID", "MovieID", "Rating", "Timestamp")

# Encode UserID and MovieID
ratings_pd = ratings.toPandas()  # Convert to pandas for encoding (if small dataset)
user_encoder = LabelEncoder()
movie_encoder = LabelEncoder()
ratings_pd['UserID'] = user_encoder.fit_transform(ratings_pd['UserID'])
ratings_pd['MovieID'] = movie_encoder.fit_transform(ratings_pd['MovieID'])

n_users = ratings_pd['UserID'].nunique()
n_movies = ratings_pd['MovieID'].nunique()

# Normalize ratings
ratings_pd['Rating'] = ratings_pd['Rating'].astype(np.float32) / 5.0

# Split data into training and test sets
from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(ratings_pd, test_size=0.2, random_state=42)

# Step 2: Create a PyTorch Dataset
class MovieLensDataset(Dataset):
    def __init__(self, data):
        self.users = torch.tensor(data['UserID'].values, dtype=torch.long)
        self.movies = torch.tensor(data['MovieID'].values, dtype=torch.long)
        self.ratings = torch.tensor(data['Rating'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        return self.users[idx], self.movies[idx], self.ratings[idx]

# Convert train and test data back to PyTorch datasets
train_dataset = MovieLensDataset(train_data)
test_dataset = MovieLensDataset(test_data)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Step 3: Define the NCF Model
class NCF(nn.Module):
    def __init__(self, n_users, n_movies, embedding_dim=32):
        super(NCF, self).__init__()
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2, 128)
        self.fc2 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 1)

    def forward(self, user, movie):
        # Get user and movie embeddings
        user_emb = self.user_embedding(user)  # Shape: [batch_size, embedding_dim]
        movie_emb = self.movie_embedding(movie)  # Shape: [batch_size, embedding_dim]

        # If user and movie dimensions do not match, expand user tensor
        if user_emb.shape[0] != movie_emb.shape[0]:
            user_emb = user_emb.expand_as(movie_emb)

        # Concatenate user and movie embeddings
        x = torch.cat([user_emb, movie_emb], dim=1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.sigmoid(self.output(x))

# Initialize the model
embedding_dim = 32
model = NCF(n_users, n_movies, embedding_dim)
criterion = nn.MSELoss()  # You can also try binary cross-entropy for implicit feedback
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Step 4: Train the Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def train_model(model, train_loader, criterion, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for user, movie, rating in train_loader:
            user, movie, rating = user.to(device), movie.to(device), rating.to(device)
            optimizer.zero_grad()
            output = model(user, movie).squeeze()
            loss = criterion(output, rating)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}")

train_model(model, train_loader, criterion, optimizer)

# Step 5: Save the Trained Model
torch.save(model.state_dict(), "ncf_model.pth")
print("Model saved as ncf_model.pth")

#with pyspark
import os
ZIP_FILE = "ml-1m.zip"
DATA_PATH = "ml-1m"

if not os.path.exists(DATA_PATH):
    os.system(f"unzip -n {ZIP_FILE} -d .")
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MovieRecommendation").getOrCreate()
movies_meta = spark.read.csv("/content/ml-1m/movies.dat", sep="::", inferSchema=True, header=False).toDF("MovieID", "Title", "Genres")

#with pyspark
from pyspark.ml.feature import CountVectorizer
from pyspark.sql.functions import col, split

# Tokenize genres
movies_meta = movies_meta.withColumn("GenresList", split(col("Genres"), "\\|"))

# Generate vector representation
vectorizer = CountVectorizer(inputCol="GenresList", outputCol="GenreVector")
model = vectorizer.fit(movies_meta)
movies_meta = model.transform(movies_meta)

#with pyspark
from pyspark.ml.linalg import Vectors
from pyspark.ml.stat import Correlation

genre_vectors = movies_meta.select("GenreVector").rdd.map(lambda row: row.GenreVector.toArray()).collect()

def get_recommendations(model, user_id, all_movies, top_k=10):
    """
    Generate movie recommendations for a given user.

    Args:
    - model: Trained NCF model.
    - user_id: User ID for which recommendations are required.
    - all_movies: List of all movie IDs in the dataset.
    - top_k: Number of recommendations to return.

    Returns:
    - recommended_movie_ids: List of recommended movie IDs.
    """
    user_tensor = torch.tensor([user_id] * len(all_movies), dtype=torch.long).to(device)
    movie_tensor = torch.tensor(all_movies, dtype=torch.long).to(device)

    with torch.no_grad():
        predictions = model(user_tensor, movie_tensor).squeeze()
    top_movie_indices = torch.topk(predictions, top_k).indices
    recommended_movie_ids = [all_movies[i] for i in top_movie_indices]
    return recommended_movie_ids

# Example usage
all_movie_ids = ratings['MovieID'].unique()
sample_user_id = 5  # Replace with any user ID
recommended_movie_ids = get_recommendations(model, sample_user_id, all_movie_ids)
print(f"Recommended Movies for User {sample_user_id}: {recommended_movie_ids}")

"""The cell print the user id along with its recommended movies"""

import os
import pandas as pd
import torch
from torch import nn
import numpy as np

# Paths to data and model
ZIP_FILE = "ml-1m.zip"
DATA_PATH = "ml-1m"
MODEL_PATH = "ncf_model.pth"

# Unzip the dataset if not already unzipped
if not os.path.exists(DATA_PATH):
    os.system(f"unzip -n {ZIP_FILE} -d .")

# Load datasets
ratings = pd.read_csv(
    os.path.join(DATA_PATH, 'ratings.dat'),
    delimiter='::',
    engine='python',
    header=None,
    names=['UserID', 'MovieID', 'Rating', 'Timestamp']
)
movies_meta = pd.read_csv(
    os.path.join(DATA_PATH, 'movies.dat'),
    delimiter='::',
    engine='python',
    header=None,
    names=['MovieID', 'Title', 'Genres'],
    encoding='ISO-8859-1'
)

# Encode UserID and MovieID
ratings['UserID'] = ratings['UserID'].astype(int)
ratings['MovieID'] = ratings['MovieID'].astype(int)

user_encoder = {u: i for i, u in enumerate(ratings['UserID'].unique())}
movie_encoder = {m: i for i, m in enumerate(ratings['MovieID'].unique())}

user_decoder = {i: u for u, i in user_encoder.items()}
movie_decoder = {i: m for m, i in movie_encoder.items()}

ratings['EncodedUserID'] = ratings['UserID'].map(user_encoder)
ratings['EncodedMovieID'] = ratings['MovieID'].map(movie_encoder)

# Define the NCF Model
class NCF(nn.Module):
    def __init__(self, n_users, n_movies, embedding_dim=32):
        super(NCF, self).__init__()
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2, 128)
        self.fc2 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 1)

    def forward(self, user, movie):
        user_emb = self.user_embedding(user)
        movie_emb = self.movie_embedding(movie)
        x = torch.cat([user_emb, movie_emb], dim=1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.sigmoid(self.output(x))

# Load the trained model
n_users = len(user_encoder)
n_movies = len(movie_encoder)

model = NCF(n_users, n_movies)
model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device("cpu")))
model.eval()

# Function to get recommendations for a user
def get_recommendations_for_user(user_id, top_n=10):
    try:
        encoded_user_id = torch.tensor([user_encoder[user_id]], dtype=torch.long)
        all_movie_ids = torch.tensor(list(movie_decoder.keys()), dtype=torch.long)

        # Predict ratings for all movies
        with torch.no_grad():
            predictions = model(encoded_user_id.expand(len(all_movie_ids)), all_movie_ids).squeeze().numpy()

        # Create a DataFrame with predictions
        recommendations = pd.DataFrame({
            'EncodedMovieID': all_movie_ids.numpy(),
            'PredictedRating': predictions
        })
        recommendations['MovieID'] = recommendations['EncodedMovieID'].map(movie_decoder)
        recommendations = recommendations.sort_values(by='PredictedRating', ascending=False).head(top_n)

        # Merge with movie metadata
        recommendations = recommendations.merge(movies_meta, on='MovieID')
        return recommendations[['Title', 'Genres', 'PredictedRating']]
    except KeyError:
        print(f"User ID {user_id} not found in the dataset.")
        return None

# Generate recommendations for all users
user_recommendations = {}
for user_id in ratings['UserID'].unique():
    print(f"Generating recommendations for User {user_id}...")
    user_recommendations[user_id] = get_recommendations_for_user(user_id)

# Display recommendations
for user_id, recs in user_recommendations.items():
    print(f"\nRecommendations for User {user_id}:")
    print(recs)

#with pyspark
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, collect_list, udf
from pyspark.sql.types import StringType
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("MovieRecommendation") \
    .getOrCreate()

# Paths to data files
DATA_PATH = "ml-1m"
MAPPED_RECOMMENDATIONS_FILE = "mapped_recommendations.csv"

# Step 1: Load Data Using Spark
movies_meta = spark.read.csv(
    os.path.join(DATA_PATH, 'movies.dat'),
    sep="::",
    inferSchema=True,
    header=False
).toDF("MovieID", "Title", "Genres")

# Step 2: Preprocessing (Filter and Fill Missing Genres)
movies_meta = movies_meta.filter(movies_meta.Genres.isNotNull())

# Step 3: Convert Genres to a Pandas DataFrame for Similarity Calculation
movies_meta_pd = movies_meta.toPandas()

# Step 4: Calculate Genre Similarity
def calculate_genre_similarity(movies_df):
    vectorizer = CountVectorizer(tokenizer=lambda x: x.split('|'))
    genre_matrix = vectorizer.fit_transform(movies_df['Genres'])
    similarity_matrix = cosine_similarity(genre_matrix)
    return similarity_matrix

similarity_matrix = calculate_genre_similarity(movies_meta_pd)

# Step 5: Generate Recommendations
def generate_recommendations(movies_df, similarity_matrix, top_n=3):
    recommendations = []
    for idx, row in movies_df.iterrows():
        movie_title = row['Title']
        movie_idx = idx
        similarity_scores = list(enumerate(similarity_matrix[movie_idx]))
        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)
        similar_movies_indices = [i[0] for i in similarity_scores if i[0] != movie_idx]

        recommended_titles = movies_df.iloc[similar_movies_indices[:top_n]]['Title'].tolist()
        recommendations.append((movie_title, '|'.join(recommended_titles)))
    return pd.DataFrame(recommendations, columns=["Title", "RecommendedTitles"])

recommendations_df = generate_recommendations(movies_meta_pd, similarity_matrix, top_n=3)

# Step 6: Save Recommendations to CSV
recommendations_df.to_csv(MAPPED_RECOMMENDATIONS_FILE, index=False)
print(f"Recommendations saved to {MAPPED_RECOMMENDATIONS_FILE}")

# Load datasets (if not already loaded)
ZIP_FILE = "ml-1m.zip"
DATA_PATH = "ml-1m"

# Unzip the dataset if necessary
if not os.path.exists(DATA_PATH):
    os.system(f"unzip -n {ZIP_FILE} -d .")

# Load movies metadata
movies_meta = pd.read_csv(
    os.path.join(DATA_PATH, 'movies.dat'),
    delimiter='::',
    engine='python',
    header=None,
    names=['MovieID', 'Title', 'Genres'],
    encoding='ISO-8859-1'
)
# Create a mapping from MovieID to Title
id_to_title = {row['MovieID']: row['Title'] for _, row in movies_meta.iterrows()}

# Replace 'recommended_movie_ids' with the IDs of movies you've predicted for a user
recommended_movie_ids = [1, 2, 3, 4, 5]  # Example IDs

# Map the movie IDs to their titles
recommended_movie_titles = [id_to_title[movie_id] for movie_id in recommended_movie_ids]
print(f"Recommended Movie Titles: {recommended_movie_titles}")

"""Now we generate a recommendations.csv file which stores MovieID , Title , genres and PredictionRatings"""

import pandas as pd
import torch
from torch import nn

# Load movie metadata
movies_meta = pd.read_csv(
    "ml-1m/movies.dat",
    delimiter='::',
    engine='python',
    header=None,
    names=['MovieID', 'Title', 'Genres'],
    encoding='ISO-8859-1'
)

# Define the NCF Model
class NCF(nn.Module):
    def __init__(self, n_users, n_movies, embedding_dim=32):
        super(NCF, self).__init__()
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2, 128)
        self.fc2 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 1)

    def forward(self, user, movie):
        user_emb = self.user_embedding(user)  # [batch_size, embedding_dim]
        movie_emb = self.movie_embedding(movie)  # [batch_size, embedding_dim]
        x = torch.cat([user_emb, movie_emb], dim=1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.sigmoid(self.output(x))

# Initialize the model with correct dimensions
n_users = 6040  # Number of unique users in the ml-1m dataset
n_movies = 3706  # Number of movies used during training
model = NCF(n_users, n_movies)

# Load the model weights
model.load_state_dict(torch.load("ncf_model.pth", map_location=torch.device("cpu")))
model.eval()

# Generate recommendations
recommendations = []
for idx, row in movies_meta.iterrows():
    if row["MovieID"] >= n_movies:  # Skip movies not in the training set
        continue

    movie_tensor = torch.tensor([row["MovieID"]], dtype=torch.long)
    user_tensor = torch.tensor([0], dtype=torch.long)  # Default user
    with torch.no_grad():
        score = model(user_tensor, movie_tensor).item()

    recommendations.append({
        "MovieID": row["MovieID"],
        "Title": row["Title"],
        "Genres": row["Genres"],
        "PredictedRating": score
    })

# Create a recommendations DataFrame
recommendations_df = pd.DataFrame(recommendations)

# Sort by predicted rating
recommendations_df = recommendations_df.sort_values(by="PredictedRating", ascending=False)

# Save recommendations to a CSV file
recommendations_df.to_csv("recommendations.csv", index=False)
print("Recommendations saved to recommendations.csv")

"""Now we generate a new csv file with mapped recomemndations which stores title and respective 10 recommended movies ."""

import os
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Paths to data files
DATA_PATH = "ml-1m"
MAPPED_RECOMMENDATIONS_FILE = "new_mapped_recommendations.csv"  # Output file

# Load movies metadata
movies_meta = pd.read_csv(
    os.path.join(DATA_PATH, 'movies.dat'),
    delimiter='::',
    engine='python',
    header=None,
    names=['MovieID', 'Title', 'Genres'],
    encoding='ISO-8859-1'
)

# Step 1: Generate a similarity matrix based on genres
def calculate_genre_similarity(movies_meta):
    vectorizer = CountVectorizer(tokenizer=lambda x: x.split('|'))
    genre_matrix = vectorizer.fit_transform(movies_meta['Genres'].fillna(''))
    similarity_matrix = cosine_similarity(genre_matrix)
    return similarity_matrix

# Step 2: Create mapped recommendations using similarity
def generate_recommendations(movies_meta, similarity_matrix, top_n=10):  # Changed top_n to 10
    mapped_recommendations = []

    for idx, row in movies_meta.iterrows():
        movie_title = row['Title']
        movie_idx = idx  # Index of the current movie in the DataFrame

        # Find the most similar movies (excluding itself)
        similarity_scores = list(enumerate(similarity_matrix[movie_idx]))
        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)
        similar_movies_indices = [i[0] for i in similarity_scores if i[0] != movie_idx]

        # Fetch titles of the top N similar movies
        recommended_titles = movies_meta.iloc[similar_movies_indices[:top_n]]['Title'].tolist()

        # Concatenate recommended titles into a single string separated by '|'
        recommended_titles_str = '|'.join(recommended_titles)

        # Append to the mapping
        mapped_recommendations.append({
            'Title': movie_title,
            'RecommendedTitles': recommended_titles_str
        })

    return mapped_recommendations

# Step 3: Compute the similarity matrix
similarity_matrix = calculate_genre_similarity(movies_meta)

# Step 4: Generate recommendations and save to CSV
recommendations = generate_recommendations(movies_meta, similarity_matrix, top_n=10)  # Changed top_n to 10
recommendations_df = pd.DataFrame(recommendations)
recommendations_df.to_csv(MAPPED_RECOMMENDATIONS_FILE, index=False)

print(f"Recommendations saved to {MAPPED_RECOMMENDATIONS_FILE}")

"""# **2. Deploying the frontend on Streamlit**"""

! pip install streamlit -q

!wget -q -O - ipv4.icanhazip.com

! streamlit run app1.py & npx localtunnel --port 8501

"""new trials

# **3. Data Visualisation using Seaborn and dask**
"""

from pyspark.sql import SparkSession
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os

# Step 1: Start Spark session
spark = SparkSession.builder.appName("MovieRecommendation").getOrCreate()

# Step 2: Load and preprocess ratings data
ZIP_FILE = "ml-1m.zip"
DATA_PATH = "ml-1m"

if not os.path.exists(DATA_PATH):
    os.system(f"unzip -n {ZIP_FILE} -d .")

# Load ratings data
ratings = spark.read.csv(
    os.path.join(DATA_PATH, 'ratings.dat'),
    sep="::",
    inferSchema=True,
    header=False
).toDF("UserID", "MovieID", "Rating", "Timestamp")

# Ensure that ratings data has consistent column names
ratings.show(5)

# Step 3: Load recommendations data
recommendations = spark.read.csv("/content/recommendations.csv", header=True, inferSchema=True)

# Ensure that recommendations data has consistent column names
recommendations.show(5)

# Step 4: Validate MovieID column
# Ensure the column names for MovieID match exactly in both DataFrames
ratings = ratings.withColumnRenamed("MovieID", "MovieID")
recommendations = recommendations.withColumnRenamed("MovieID", "MovieID")

# Step 5: Join the data on MovieID
ratings_similarity = ratings.join(recommendations, on="MovieID", how="inner")
ratings_similarity.show(5)

# Step 6: Convert to pandas for visualization
ratings_similarity_pd = ratings_similarity.select("Rating", "PredictedRating").toPandas()

# Step 7: Visualize the correlation
sns.scatterplot(x="Rating", y="PredictedRating", data=ratings_similarity_pd, color='purple')
plt.title('Correlation Between Ratings and Predicted Similarity Scores')
plt.xlabel('User Ratings')
plt.ylabel('Predicted Similarity Scores')
plt.show()

pip install dask[complete]

import dask.dataframe as dd

# Load ratings data with Dask
ratings = dd.read_csv(
    "ml-1m/ratings.dat",
    sep="::",
    header=None,
    names=["UserID", "MovieID", "Rating", "Timestamp"],
    engine="python"
)

# Load recommendations data with Dask
recommendations = dd.read_csv(
    "recommendations.csv",
    engine="python"  # If necessary
)

# Merge ratings and recommendations data
ratings_recommendations = ratings.merge(recommendations, on="MovieID")

# Perform filtering or other operations (if needed)
filtered_data = ratings_recommendations[ratings_recommendations["Rating"] > 3.0]

# Compute results (only when you need the actual data)
result = filtered_data.compute()

# Calculate average rating for each movie
average_ratings = ratings.groupby("MovieID")["Rating"].mean().compute()

# Calculate the number of ratings per movie
rating_counts = ratings.groupby("MovieID")["Rating"].count().compute()

import dask.dataframe as dd
import matplotlib.pyplot as plt
import pandas as pd

# Step 1: Load data into Dask DataFrame
ratings_path = "ml-1m/ratings.dat"
movies_path = "ml-1m/movies.dat"

ratings = dd.read_csv(
    ratings_path, sep="::", header=None, engine="python", names=["UserID", "MovieID", "Rating", "Timestamp"]
)

movies = pd.read_csv(
    movies_path, sep="::", header=None, engine="python", names=["MovieID", "Title", "Genres"], encoding="ISO-8859-1"
)

# Step 2: Compute average ratings per movie
average_ratings = ratings.groupby("MovieID")["Rating"].mean()

# Convert the Dask Series to pandas DataFrame
average_ratings_pd = average_ratings.compute().reset_index()

# Step 3: Join with movies metadata to get movie titles
average_ratings_pd = average_ratings_pd.merge(movies, on="MovieID", how="inner")

# Optional: Sort by average ratings and limit to top 10 movies for better visualization
average_ratings_pd = average_ratings_pd.sort_values(by="Rating", ascending=False).head(10)

# Step 4: Plot the data
plt.figure(figsize=(10, 5))
plt.bar(average_ratings_pd["Title"], average_ratings_pd["Rating"], color="purple")
plt.title("Top 10 Movies with Highest Average Ratings")
plt.xlabel("Movie Title")
plt.ylabel("Average Rating")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Ensure ratings is a Dask DataFrame and compute it
ratings_pd = ratings.compute()

# Explode genres into multiple rows
movies_genres = movies.copy()
movies_genres["Genres"] = movies_genres["Genres"].fillna("")
movies_genres = movies_genres.assign(Genres=movies_genres["Genres"].str.split("|")).explode("Genres")

# Join with ratings to get genre-level rating counts
ratings_with_genres = ratings_pd.merge(movies_genres, on="MovieID", how="inner")

# Group by genres and count ratings
genre_counts = ratings_with_genres.groupby("Genres")["Rating"].count().reset_index(name="RatingCount")

# Sort and plot the data
genre_counts = genre_counts.sort_values(by="RatingCount", ascending=False)

plt.figure(figsize=(10, 5))
plt.bar(genre_counts["Genres"], genre_counts["RatingCount"], color="salmon")
plt.title("Number of Ratings Per Genre")
plt.xlabel("Genre")
plt.ylabel("Number of Ratings")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import dask.dataframe as dd

# Explode genres into multiple rows
movies_genres = movies.copy()
movies_genres["Genres"] = movies_genres["Genres"].fillna("")
movies_genres = movies_genres.assign(Genres=movies_genres["Genres"].str.split("|")).explode("Genres")

# Ensure both ratings and movies_genres are Dask DataFrames
movies_genres_dd = dd.from_pandas(movies_genres, npartitions=4)
ratings_dd = dd.from_pandas(ratings_pd, npartitions=4)  # Convert ratings_pd to a Dask DataFrame

# Merge ratings and genres
ratings_with_genres_dd = ratings_dd.merge(movies_genres_dd, on="MovieID", how="inner")

# Calculate top-rated movies by genre
top_movies_by_genre_dd = (
    ratings_with_genres_dd.groupby(["Genres", "Title"])["Rating"]
    .count()
    .reset_index()  # Convert groupby object to a DataFrame
    .rename(columns={"Rating": "RatingCount"})  # Rename the aggregated column to RatingCount
)

# Compute the result
top_movies_by_genre_pd = top_movies_by_genre_dd.compute()

# Sort the results after computation
top_movies_by_genre_sorted = top_movies_by_genre_pd.sort_values(
    ["Genres", "RatingCount"], ascending=[True, False]
)

# Display the top movies for each genre
for genre in top_movies_by_genre_sorted["Genres"].unique():
    print(f"\nTop Movies in Genre: {genre}")
    print(top_movies_by_genre_sorted[top_movies_by_genre_sorted["Genres"] == genre].head(5))

import matplotlib.pyplot as plt

# Visualize the top movies by genre
unique_genres = top_movies_by_genre_sorted["Genres"].unique()

# Plot each genre separately
for genre in unique_genres:
    # Filter top 5 movies for the current genre
    genre_data = top_movies_by_genre_sorted[top_movies_by_genre_sorted["Genres"] == genre].head(5)

    # Plot the data
    plt.figure(figsize=(10, 5))
    plt.barh(genre_data["Title"], genre_data["RatingCount"], color="skyblue")
    plt.title(f"Top 5 Movies in Genre: {genre}")
    plt.xlabel("Number of Ratings")
    plt.ylabel("Movie Title")
    plt.gca().invert_yaxis()  # Invert y-axis for better visualization
    plt.tight_layout()
    plt.show()

